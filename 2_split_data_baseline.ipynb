{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciPy imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Other necessary imports\n",
    "import sqlite3\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened database successfully\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('database.sqlite')\n",
    "cursor = conn.cursor()\n",
    "print(\"Opened database successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy\n",
    "\n",
    "- subset to last 10 days of searches (timeframe for testSearchStream + 1 day)\n",
    "- select last 7 searches per User for val/test\n",
    "- historical searches are everything prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** any query beginning with \"CREATE TABLE\" was compiled in batch_create_tables.py\n",
    "\n",
    "*See README.md for more information*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrict Timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 34s, sys: 5min 46s, total: 14min 21s\n",
      "Wall time: 32min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1a333a0d50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## creating a table with the last 10 days of searches\n",
    "q = '''\n",
    "CREATE TABLE last10days AS\n",
    "    SELECT * FROM SearchInfo2\n",
    "    WHERE SearchDate > \"2015-05-11 00:00:00.0\"\n",
    "    ORDER BY SearchDate DESC;\n",
    "'''\n",
    "%time cursor.executescript(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take the search IDs from above query and grab the related impressions\n",
    "q ='''\n",
    "CREATE TABLE last10days_imp AS\n",
    "    SELECT * FROM trainSearchStream\n",
    "    WHERE SearchID IN (\n",
    "    SELECT SearchID FROM last10days \n",
    "    ORDER BY SearchID ASC);\n",
    "'''\n",
    "%time cursor.executescript(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct SearchIDs from last9days:  39009184\n",
      "Distinct SearchIDs from last9days of impressions:  39009184\n"
     ]
    }
   ],
   "source": [
    "# compare count of SearchIDs from last10days (of SearchInfo)\n",
    "# to count of SearchIDs when the correlated impressions are queried\n",
    "q1 = '''\n",
    "SELECT COUNT(DISTINCT(SearchID))\n",
    "FROM last10days'''\n",
    "q2 = '''\n",
    "SELECT COUNT(DISTINCT(SearchID))\n",
    "FROM last10days_imp'''\n",
    "print('Distinct SearchIDs from last10days: ', cursor.execute(q1).fetchone()[0])\n",
    "print('Distinct SearchIDs from last10days of impressions: ', cursor.execute(q2).fetchone()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets correlated Ad information for ads in last10days_imp\n",
    "q ='''\n",
    "CREATE TABLE last10days_ads AS\n",
    "    SELECT * FROM AdsInfo\n",
    "    WHERE AdID IN (\n",
    "    SELECT AdID FROM last10days_imp \n",
    "    ORDER BY AdID ASC);\n",
    "'''\n",
    "%time cursor.executescript(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 14s, sys: 1h 1min 23s, total: 1h 20min 37s\n",
      "Wall time: 1h 46min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1a333a0d50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge all those tables together for convenience\n",
    "q ='''\n",
    "CREATE TABLE last10days_merged AS\n",
    "\n",
    "    SELECT i.*, a.*, s.*\n",
    "    FROM last10days_imp i\n",
    "    JOIN (\n",
    "          SELECT * FROM last10days \n",
    "         ) s\n",
    "    ON s.SearchID = i.SearchID\n",
    "    JOIN last10days_ads a\n",
    "    ON a.AdID = i.AdID;\n",
    "'''\n",
    "%time cursor.executescript(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 57s, sys: 5min 59s, total: 10min 56s\n",
      "Wall time: 17min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1c3176ef10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get history of landing page visits for Users in last9days\n",
    "q ='''\n",
    "CREATE TABLE hist_visits AS\n",
    "    SELECT * FROM VisitsStream\n",
    "    WHERE ViewDate > \"2015-05-11 00:00:00.0\"\n",
    "    AND   UserID IN (\n",
    "            SELECT UserID FROM last10days);\n",
    "'''\n",
    "%time cursor.executescript(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating Validation & Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 38s, sys: 1min 11s, total: 2min 50s\n",
      "Wall time: 5min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1a714f3030>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## for the last 7 days of Searches,\n",
    "## rank Searches per User in reverse order\n",
    "last_seven_searches = '''\n",
    "CREATE TABLE last_seven_searches AS\n",
    "\n",
    "SELECT last_seven.*\n",
    "FROM\n",
    "    (SELECT UserID, SearchID, SearchDate,\n",
    "         row_number() OVER (PARTITION BY UserID ORDER BY SearchDate DESC) AS row_n\n",
    "     FROM last10days) last_seven\n",
    "WHERE row_n <= 7\n",
    ";\n",
    "'''\n",
    "%time cursor.execute(last_seven_searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 54.1 s, total: 2min 31s\n",
      "Wall time: 3min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1a714f3030>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the last 7 days of Searches,\n",
    "## rank Searches per User in reverse order\n",
    "# take all searches after 7th-to-last and lump together as \"historical\"\n",
    "historical_searches = '''\n",
    "CREATE TABLE hist_searches AS\n",
    "\n",
    "SELECT last_seven.*\n",
    "FROM\n",
    "    (SELECT UserID, SearchID, SearchDate,\n",
    "         row_number() OVER (PARTITION BY UserID ORDER BY SearchDate DESC) AS row_n\n",
    "     FROM last10days) last_seven\n",
    "WHERE row_n > 7\n",
    ";\n",
    "'''\n",
    "%time cursor.execute(historical_searches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of UserIds for search# 7:  1091581\n",
      "Number of UserIds for search# 6:  1209443\n",
      "Number of UserIds for search# 5:  1356606\n",
      "Number of UserIds for search# 4:  1542396\n",
      "Number of UserIds for search# 3:  1790899\n",
      "Number of UserIds for search# 2:  2156078\n",
      "Number of UserIds for search# 1:  2756369\n"
     ]
    }
   ],
   "source": [
    "# print count of UserIDs in each round of searches for Last Searches #1-7 (in reverse)\n",
    "for n in range(7,0,-1):\n",
    "    q = f'''\n",
    "    SELECT COUNT(DISTINCT(UserID)) FROM last_seven_searches\n",
    "    WHERE row_n = ?;\n",
    "    '''\n",
    "    print(f'Number of UserIds for search# {n}: ', cursor.execute(q, (n,)).fetchone()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Every User has at least 1 search, with those having more-than-one decreasing in quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Train/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building Training/Validation Set\n",
    "train = '''\n",
    "SELECT \n",
    "    m.SearchID, AdID, m.UserID, m.SearchDate,\n",
    "    Price, Position, LENGTH(Title) title_length, \n",
    "    IsClick, IsContext, IsUserLoggedOn,\n",
    "    CategoryID CategoryID_s,\n",
    "    \"CategoryID:1\" CategoryID_a,\n",
    "    Params, SearchParams, SearchQuery,\n",
    "    tr.row_n\n",
    "FROM last10days_merged m\n",
    "\n",
    "JOIN last_seven_searches tr\n",
    "ON m.SearchID = tr.SearchID\n",
    "-- only take impressions from the seventh-to-last to second-to-last search per User\n",
    "WHERE IsContext = 1 AND\n",
    "      tr.row_n > 1  AND\n",
    "      m.SearchDate >= \"2015-05-12 00:00:00.0\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building holdout Test Set\n",
    "test = '''\n",
    "SELECT \n",
    "    m.SearchID, AdID, m.UserID,  m.SearchDate,\n",
    "    Price, Position, LENGTH(Title) title_length, \n",
    "    IsClick, IsContext, IsUserLoggedOn,\n",
    "    CategoryID CategoryID_s,\n",
    "    \"CategoryID:1\" CategoryID_a,\n",
    "    Params, SearchParams, SearchQuery,\n",
    "    tr.row_n\n",
    "FROM last10days_merged m\n",
    "\n",
    "JOIN last_seven_searches tr\n",
    "ON m.SearchID = tr.SearchID\n",
    "\n",
    "-- only take impressions from the last search per User\n",
    "WHERE IsContext = 1 AND\n",
    "      tr.row_n = 1 AND\n",
    "      m.SearchDate >= \"2015-05-12 00:00:00.0\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 40s, sys: 4min 44s, total: 11min 24s\n",
      "Wall time: 16min 14s\n"
     ]
    }
   ],
   "source": [
    "%time train_raw = pd.read_sql_query(train, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 30s, sys: 1min 59s, total: 4min 29s\n",
      "Wall time: 7min 44s\n"
     ]
    }
   ],
   "source": [
    "%time test_raw = pd.read_sql_query(test, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle it!\n",
    "train_raw.to_pickle(open('pickle/train_raw.pkl.gzip', 'wb'), compression='gzip')\n",
    "test_raw.to_pickle(open('pickle/test_raw.pkl.gzip', 'wb'), compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(df, components, col_name):\n",
    "    '''\n",
    "    Builds a dictionary of DataFrame indices split into a Walk-Forward Validation\n",
    "    scheme. Each \"step\" or split comprises of an initial \"train_set\" for model fitting, \n",
    "    a \"val_set\" for model evaluation, and an \"agg_set_v\" which is the historical data that\n",
    "    will be merged onto the \"val_set\".  After the first split, the remaining splits will \n",
    "    also have a \"agg_set_t\" - the historical data that will be merged into the training set.\n",
    "    \n",
    "    For example, with 2 splits of search items [3rd-last, 2nd-last, last]:\n",
    "    ROUND 1: \n",
    "    Model is Fit on 'Train Set 1' (3rd-last) \n",
    "    Model is Scored on 'Val Set 1' (2nd-last) \n",
    "                        w/Agg Set V1 (3rd-last) as supporting historical data\n",
    "    \n",
    "    ROUND 2:\n",
    "    Model is Fit on 'Train Set 2' (2nd-last)\n",
    "                        w/ Agg Set T2 (3rd-last) as supporting historical data\n",
    "    Model is Scored on 'Val Set 2' (last)\n",
    "                        w/ Agg Set V2 (2nd-last) as supporting historical data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Data to be split.\n",
    "    \n",
    "    components : list of integers\n",
    "        Sequence of time-series events, listed as reverse-ordinal events.\n",
    "            e.g. [3, 2, 1] for third-to-last, second-to-last, last\n",
    "        \n",
    "    col_name : string\n",
    "        Column name that references the time-series event\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    train_val_splits \n",
    "        Dictionary of indices corresponding to splits.\n",
    "        Number of splits is 1 - len(components).\n",
    "    '''\n",
    "    # instantiate defaultdict to contain splits\n",
    "    train_val_splits = defaultdict()\n",
    "    # loop over number of splits (number of components - 1)\n",
    "    for i in range(len(components)-1):\n",
    "        # for first split, train set & agg_set for validation will be the same\n",
    "        # no explicit agg_set for training \n",
    "        if i == 0:\n",
    "            train_val_splits[f'train_set_{i+1}'] = df[df[col_name] == components[i]].index.values\n",
    "            train_val_splits[f'agg_set_v{i+1}'] = df[df[col_name] > components[i+1]].index.values\n",
    "            train_val_splits[f'val_set_{i+1}'] = df[df[col_name] == components[i+1]].index.values\n",
    "        else:\n",
    "            # agg_set_t takes all indices prior to train_set indices\n",
    "            train_val_splits[f'agg_set_t{i+1}'] = df[df[col_name] > components[i]].index.values\n",
    "            # train_set takes all indices at component\n",
    "            train_val_splits[f'train_set_{i+1}'] = df[df[col_name] == components[i]].index.values\n",
    "            # agg_set_v takes all indices prior to val_set indices\n",
    "            train_val_splits[f'agg_set_v{i+1}'] = df[df[col_name] > components[i+1]].index.values\n",
    "            # val_set takes all indices at subsequent component\n",
    "            train_val_splits[f'val_set_{i+1}'] = df[df[col_name] == components[i+1]].index.values\n",
    "            \n",
    "    return train_val_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index splits for training/validation\n",
    "train_val_dict = train_val_split(train_raw, components=[7,6,5,4,3,2], col_name='row_n')\n",
    "# pickle it\n",
    "pickle.dump(train_val_dict, open('pickle/train_val_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First split train_set and agg_set_v1 equivalent:  True\n",
      "Training Set 3 is the same as Validation Set 2:  True\n",
      "Agg Set for Training 4 is the same as Agg Set for Validation 3:  True\n"
     ]
    }
   ],
   "source": [
    "## statements should all equal 'True'\n",
    "check1 = np.array_equal(train_val_dict['train_set_1'], train_val_dict['agg_set_v1'])\n",
    "print('First split train_set and agg_set_v1 equivalent: ', check1)\n",
    "\n",
    "check2 = np.array_equal(train_val_dict['val_set_2'], train_val_dict['train_set_3'])\n",
    "print('Training Set 3 is the same as Validation Set 2: ', check2)\n",
    "\n",
    "check3 = np.array_equal(train_val_dict['agg_set_t4'], train_val_dict['agg_set_v3'])\n",
    "print('Agg Set for Training 4 is the same as Agg Set for Validation 3: ', check3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast Price column as numeric and replace empty values with NaNs\n",
    "def make_price_numeric(df):\n",
    "    df['Price'] = df['Price'].replace('', np.NaN)\n",
    "    return df.astype({'Price': 'float'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_baseline = make_price_numeric(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SearchID            int64\n",
       "AdID                int64\n",
       "UserID              int64\n",
       "SearchDate         object\n",
       "Price             float64\n",
       "Position            int64\n",
       "title_length        int64\n",
       "IsClick             int64\n",
       "IsContext           int64\n",
       "IsUserLoggedOn      int64\n",
       "CategoryID_s       object\n",
       "CategoryID_a        int64\n",
       "Params             object\n",
       "SearchParams       object\n",
       "SearchQuery        object\n",
       "row_n               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_baseline.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic features and target\n",
    "features = ['Price', 'Position', 'title_length', 'CategoryID_a']\n",
    "target = ['IsClick']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset training/validation DataFrame to only contain features/target columns\n",
    "train_baseline = train_baseline[features + target]\n",
    "# some null prices need to have an imputed value - using 0 for simplicity\n",
    "train_baseline['Price'] = train_baseline['Price'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "X_train.shape:  (1526110, 4)\n",
      "X_val.shape:  (1707463, 4)\n",
      "Baseline AUC = 0.6477311237604879 \n",
      "\n",
      "Round 2\n",
      "X_train.shape:  (1707463, 4)\n",
      "X_val.shape:  (1934930, 4)\n",
      "Baseline AUC = 0.6502892716940246 \n",
      "\n",
      "Round 3\n",
      "X_train.shape:  (1934930, 4)\n",
      "X_val.shape:  (2226560, 4)\n",
      "Baseline AUC = 0.657265183533147 \n",
      "\n",
      "Round 4\n",
      "X_train.shape:  (2226560, 4)\n",
      "X_val.shape:  (2619364, 4)\n",
      "Baseline AUC = 0.666012287642193 \n",
      "\n",
      "Round 5\n",
      "X_train.shape:  (2619364, 4)\n",
      "X_val.shape:  (3196045, 4)\n",
      "Baseline AUC = 0.6757942967677281 \n",
      "\n",
      "***************\n",
      "Mean AUC over 5-rounds: 0.659418432679516\n"
     ]
    }
   ],
   "source": [
    "# set empty array for auc_scores - we will later take the mean of all 5 rounds of validation\n",
    "auc_scores = np.zeros(5)\n",
    "\n",
    "for i in range(1, 6):\n",
    "    \n",
    "    # get indices for splits\n",
    "    t_index = train_val_dict[f'train_set_{i}']\n",
    "    v_index = train_val_dict[f'val_set_{i}']\n",
    "    \n",
    "    # filter full train_df for splits\n",
    "    t = train_baseline.iloc[t_index]\n",
    "    v = train_baseline.iloc[v_index]\n",
    "    \n",
    "    # scale/encode features\n",
    "    preprocess = make_column_transformer(\n",
    "        (['Price', 'title_length'], StandardScaler()),\n",
    "        (['Position', 'CategoryID_a'], OneHotEncoder(categories='auto', \n",
    "                                                     handle_unknown='ignore'))\n",
    "                                        )\n",
    "\n",
    "    # split train into features(X) and target(y)\n",
    "    X_t = t.drop(columns=['IsClick'])\n",
    "    X_ts = preprocess.fit_transform(X_t)\n",
    "    y_t = t.IsClick\n",
    "    \n",
    "    # split val into features(X) and target(y)\n",
    "    X_v = v.drop(columns=['IsClick'])\n",
    "    X_vs = preprocess.transform(X_v)\n",
    "    y_v = v.IsClick\n",
    "    \n",
    "    # log round # and train/val dataframe shapes\n",
    "    print(f'Round {i}')\n",
    "    print('X_train.shape: ', X_t.shape)\n",
    "    print('X_val.shape: ', X_v.shape)\n",
    "    \n",
    "    # instantiate baseline model\n",
    "    m = LogisticRegression(solver='liblinear')\n",
    "    # fit on scaled/encoded training set\n",
    "    m.fit(X_ts, y_t)\n",
    "    # predict probability of target class for scaled/encoded validation set\n",
    "    preds = m.predict_proba(X_vs)[:,1]\n",
    "    # get AUC score\n",
    "    auc = roc_auc_score(y_v, preds)\n",
    "    # insert AUC score in numpy array\n",
    "    auc_scores[i-1] = auc\n",
    "    # print AUC score for current round\n",
    "    print(f'Baseline AUC = {auc}', '\\n')\n",
    "\n",
    "# print final mean AUC for all validation rounds\n",
    "print('*' * 15)\n",
    "print(f'Mean AUC over 5-rounds: {np.mean(auc_scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you wanted to test baseline on Kaggle:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all last 7 searches for each User\n",
    "tr = pd.concat([train_raw, test_raw])\n",
    "# convert Price column to numeric\n",
    "tr = make_price_numeric(tr)\n",
    "# subset DataFrame to only include 'features' and 'target' columns\n",
    "tr = tr[features + target]\n",
    "# replace NaN prices with 0\n",
    "tr['Price'] = tr['Price'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in kaggle test set from sql\n",
    "q = '''\n",
    "SELECT TestID, AdID, Price, Position, \n",
    "       LENGTH(Title) title_length, \n",
    "       \"CategoryID:1\" CategoryID_a\n",
    "FROM test_merged\n",
    "'''\n",
    "te = pd.read_sql_query(q, conn)\n",
    "# convert Price column to numeric\n",
    "te = make_price_numeric(te)\n",
    "# replace NaN prices with 0\n",
    "te['Price'] = te['Price'].fillna(0)\n",
    "# some categories in this column were Null so we have to add this arbitrary value\n",
    "te['CategoryID_a'] = te['CategoryID_a'].replace('', '-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess, Model, Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale encode/columns like before\n",
    "preprocess = make_column_transformer(\n",
    "    (['Price', 'title_length'], StandardScaler()),\n",
    "    (['Position', 'CategoryID_a'], OneHotEncoder(categories='auto', handle_unknown='ignore'))\n",
    ")\n",
    "\n",
    "# split train into features(X) and target(y)\n",
    "X_t = tr.drop(columns=['IsClick'])\n",
    "X_ts = preprocess.fit_transform(X_t)\n",
    "y_t = tr.IsClick\n",
    "\n",
    "# split test into features(X) NOTE: kaggle test_set has no target included - that's what we're predicting\n",
    "X_te = te\n",
    "X_tes = preprocess.transform(X_te)\n",
    "\n",
    "# train model and predict probability of IsClick for test\n",
    "m = LogisticRegression(solver='liblinear')\n",
    "m.fit(X_ts, y_t)\n",
    "preds = m.predict_proba(X_tes)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set predictions to 'IsClick' column\n",
    "X_te['IsClick'] = preds\n",
    "# write to csv - this is the correct format for submission\n",
    "X_te[['TestId', 'IsClick']].to_csv(open('submission.csv', 'w'), header=['ID', 'IsClick'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Keep in mind that we've been using AUC to evaluate rather than the competition metric - LogLoss.**\n",
    "\n",
    "That said, the above gets you just under 50th percentile or so submissions on the Private Leaderboard with a LogLoss ~0.04982.  You'll see all the other Logistic Regression scripts around there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
