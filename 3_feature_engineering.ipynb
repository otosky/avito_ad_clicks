{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciPy imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Machine Learning related\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# more models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Other necessary imports\n",
    "import sqlite3\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "import time\n",
    "\n",
    "# just to eliminate some annoying sklearn warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened database successfully\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('database.sqlite')\n",
    "cursor = conn.cursor()\n",
    "print(\"Opened database successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Train/Val/Test Data From Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_pickle(open('pickle/train_raw.pkl.gzip', 'rb'), compression='gzip')\n",
    "test_raw = pd.read_pickle(open('pickle/test_raw.pkl.gzip', 'rb'), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dict = pickle.load(open('pickle/train_val_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matching Search Params\n",
    "\n",
    "# extract Search Params numbers for search\n",
    "def s_params_keys(string):\n",
    "    try:\n",
    "        d = ast.literal_eval(string)\n",
    "        return set(x for x in d.keys())\n",
    "    except:\n",
    "        return set('s')\n",
    "# extract Ad Params numbers from Ad\n",
    "def a_params_keys(string):\n",
    "    try:\n",
    "        d = ast.literal_eval(string)\n",
    "        return set(x for x in d.keys())\n",
    "    except:\n",
    "        return set('a')\n",
    "\n",
    "# compare set of Search Params & Ad Params and get count of intersection\n",
    "def get_common_params(df):\n",
    "    \n",
    "    # TODO: try to find an alternative for apply function cuz it'sa slooow\n",
    "    s_params_set = df['SearchParams'].apply(s_params_keys)\n",
    "    a_params_set = df['Params'].apply(a_params_keys)\n",
    "    common_params_cnt = s_params_set.values & a_params_set.values\n",
    "    len_vectorized = np.frompyfunc(len, 1, 1)\n",
    "    return len_vectorized(common_params_cnt)\n",
    "\n",
    "# checks to see if CategoryID for Search and Ad match for a given impression\n",
    "def matched_categories(df):\n",
    "    \n",
    "    categories_match = df['CategoryID_s'] == df['CategoryID_a']\n",
    "    return categories_match.astype(int)\n",
    "\n",
    "# checks to see if Search Query was left blank for a given Search\n",
    "def query_blank(df):\n",
    "    \n",
    "    blank_query = (df['SearchQuery'] == '').astype(int)\n",
    "    return blank_query\n",
    "\n",
    "# cast Price column as numeric and replace empty values with NaNs\n",
    "def make_price_numeric(df):\n",
    "    price = df['Price'].replace('', np.NaN)\n",
    "    df.astype({'Price': 'float'})\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidates all the above preprocessing functions\n",
    "def basic_cleaning(df):\n",
    "    # convert SearchDate to datetime\n",
    "    df['SearchDate'] = pd.to_datetime(df['SearchDate'])\n",
    "    \n",
    "    # get the common parameters between SearchID and AdID\n",
    "    df['common_params_cnt'] = get_common_params(df)\n",
    "    print('Finished get_common_params')\n",
    "    \n",
    "    # do the categories match between Search and Ad?\n",
    "    df['categories_match'] = matched_categories(df)\n",
    "    # replace blank categories with arbitrary negative number\n",
    "    df['CategoryID_s'] = df['CategoryID_s'].replace('', -1)\n",
    "    df['CategoryID_a'] = df['CategoryID_a'].replace('', -2)\n",
    "    print('Finished matched_categories')\n",
    "    \n",
    "    # was the search query blank?\n",
    "    df['blank_query'] = query_blank(df)\n",
    "    print('Finished query_blank')\n",
    "    \n",
    "    # convert the price column to float and fill '' with NaN\n",
    "    df['Price'] = make_price_numeric(df)\n",
    "    print('Finished make_price_numeric')\n",
    "    \n",
    "    # drop columns that are now irrelevant\n",
    "    df.drop(columns=['Params', 'SearchParams', 'SearchQuery', 'IsContext'], inplace=True)\n",
    "    \n",
    "    # return DataFrame with mentioned columns as numeric\n",
    "    return df.astype({'Price': 'float', 'common_params_cnt': 'int', \n",
    "                      'CategoryID_s': 'int', 'CategoryID_a': 'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply to Train & Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished get_common_params\n",
      "Finished matched_categories\n",
      "Finished query_blank\n",
      "Finished make_price_numeric\n",
      "CPU times: user 10min 11s, sys: 7min 38s, total: 17min 49s\n",
      "Wall time: 20min 37s\n"
     ]
    }
   ],
   "source": [
    "%time train_raw = basic_cleaning(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished get_common_params\n",
      "Finished matched_categories\n",
      "Finished query_blank\n",
      "Finished make_price_numeric\n",
      "CPU times: user 2min 33s, sys: 16 s, total: 2min 49s\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%time test_raw = basic_cleaning(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SearchID</th>\n",
       "      <th>AdID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>SearchDate</th>\n",
       "      <th>Price</th>\n",
       "      <th>Position</th>\n",
       "      <th>title_length</th>\n",
       "      <th>IsClick</th>\n",
       "      <th>IsUserLoggedOn</th>\n",
       "      <th>CategoryID_s</th>\n",
       "      <th>CategoryID_a</th>\n",
       "      <th>row_n</th>\n",
       "      <th>common_params_cnt</th>\n",
       "      <th>categories_match</th>\n",
       "      <th>blank_query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>13524889</td>\n",
       "      <td>3310798</td>\n",
       "      <td>2015-05-15 17:38:46</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>34084553</td>\n",
       "      <td>3310798</td>\n",
       "      <td>2015-05-15 17:38:46</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>12281759</td>\n",
       "      <td>3524741</td>\n",
       "      <td>2015-05-13 09:58:44</td>\n",
       "      <td>40086.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>29614929</td>\n",
       "      <td>4114490</td>\n",
       "      <td>2015-05-15 09:56:17</td>\n",
       "      <td>9829.0</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>19880051</td>\n",
       "      <td>3314799</td>\n",
       "      <td>2015-05-18 08:46:10</td>\n",
       "      <td>17100.0</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SearchID      AdID   UserID          SearchDate    Price  Position  \\\n",
       "0        10  13524889  3310798 2015-05-15 17:38:46   5000.0         1   \n",
       "1        10  34084553  3310798 2015-05-15 17:38:46  30000.0         7   \n",
       "2        23  12281759  3524741 2015-05-13 09:58:44  40086.0         1   \n",
       "3        25  29614929  4114490 2015-05-15 09:56:17   9829.0         1   \n",
       "4        37  19880051  3314799 2015-05-18 08:46:10  17100.0         7   \n",
       "\n",
       "   title_length  IsClick  IsUserLoggedOn  CategoryID_s  CategoryID_a  row_n  \\\n",
       "0            38        0               0            34            12      5   \n",
       "1            12        0               0            34            12      5   \n",
       "2            40        0               0            38            38      5   \n",
       "3            47        0               0            47            47      5   \n",
       "4            28        0               0            11            11      3   \n",
       "\n",
       "   common_params_cnt  categories_match  blank_query  \n",
       "0                  0                 0            1  \n",
       "1                  0                 0            1  \n",
       "2                  1                 1            1  \n",
       "3                  0                 1            0  \n",
       "4                  0                 1            1  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview output\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can pickle that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw.to_pickle(open('pickle/train_processed.pkl.gzip', 'wb'), compression='gzip')\n",
    "del train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw.to_pickle(open('pickle/test_processed.pkl.gzip', 'wb'), compression='gzip')\n",
    "del test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign pickles\n",
    "# also you can restart from here if your notebook kernel dies for whatever reason\n",
    "train_processed = pd.read_pickle(open('pickle/train_processed.pkl.gzip', 'rb'), compression='gzip')\n",
    "test_processed = pd.read_pickle(open('pickle/test_processed.pkl.gzip', 'rb'), compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*If you think you might want to submit to Kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kaggle_test():\n",
    "    \n",
    "    # query database for kaggle test set\n",
    "    q = '''\n",
    "        SELECT TestID, SearchID, AdID, UserID, SearchDate,\n",
    "               Price, Position, LENGTH(Title) title_length, \n",
    "               IsContext, IsUserLoggedOn,\n",
    "               CategoryID CategoryID_s,\n",
    "               \"CategoryID:1\" CategoryID_a,\n",
    "               Params, SearchParams, SearchQuery\n",
    "        FROM test_merged\n",
    "        '''\n",
    "    kaggle = pd.read_sql_query(q, conn)\n",
    "    # apply basic cleaning functions to kaggle DataFrame\n",
    "    kaggle = basic_cleaning(kaggle)\n",
    "    \n",
    "    return kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished get_common_params\n",
      "Finished matched_categories\n",
      "Finished query_blank\n",
      "Finished make_price_numeric\n",
      "CPU times: user 6min 47s, sys: 3min 55s, total: 10min 43s\n",
      "Wall time: 13min 31s\n"
     ]
    }
   ],
   "source": [
    "%time kaggle = get_kaggle_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can pickle that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle and delete variable to save memory for now\n",
    "kaggle.to_pickle(open('pickle/kaggle_processed.pkl.gzip', 'wb'), compression='gzip')\n",
    "del kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 46s, sys: 8min 52s, total: 16min 39s\n",
      "Wall time: 23min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1a345648f0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## aggregates User Features for Searches preceding last7searches\n",
    "hist_user_agg = '''\n",
    "CREATE TABLE hist_user_agg AS\n",
    "\n",
    "WITH historical_impressions AS\n",
    "(SELECT m.SearchID, AdID, UserID, IsClick, IsContext, CategoryID CategoryID_s\n",
    "FROM last10days_merged m\n",
    "WHERE m.SearchID IN\n",
    "    (SELECT SearchID \n",
    "     FROM hist_searches)\n",
    ")\n",
    "\n",
    "SELECT a.UserID,\n",
    "       user_total_searches,\n",
    "       user_category_counts,\n",
    "       user_total_clicks,\n",
    "       user_total_impressions \n",
    "\n",
    "FROM (  SELECT UserID,\n",
    "               COUNT(SearchID) user_total_searches,\n",
    "               COUNT(DISTINCT(CategoryID_s)) user_category_counts,\n",
    "               SUM(IsClick) user_total_clicks\n",
    "        FROM historical_impressions\n",
    "        GROUP BY UserID\n",
    "        ) AS a\n",
    "\n",
    "JOIN ( SELECT UserID,\n",
    "               COUNT(AdID) user_total_impressions\n",
    "        FROM historical_impressions\n",
    "        WHERE IsContext = 1\n",
    "        GROUP BY UserID \n",
    "        ) AS b\n",
    "    ON a.UserID = b.UserID\n",
    "ORDER BY a.UserID\n",
    ";\n",
    "'''\n",
    "%time cursor.executescript(hist_user_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count =  993106\n",
      "Count =  993106\n",
      "Count =  993106\n"
     ]
    }
   ],
   "source": [
    "## sanity check to make sure no Users got duplicated - these queries should all return the same # of counts\n",
    "q1 = '''\n",
    "SELECT (COUNT(DISTINCT(UserID))) FROM hist_user_agg;\n",
    "'''\n",
    "\n",
    "q2 = '''\n",
    "SELECT COUNT(*) FROM hist_user_agg;\n",
    "'''\n",
    "\n",
    "q3 = '''\n",
    "WITH hist_imp AS\n",
    "(SELECT m.SearchID, AdID, m.UserID, IsClick, IsContext, CategoryID CategoryID_s\n",
    "FROM last10days_merged m\n",
    "WHERE m.SearchID IN\n",
    "    (SELECT SearchID \n",
    "     FROM hist_searches)\n",
    ")\n",
    "\n",
    "SELECT COUNT(DISTINCT(UserID)) FROM hist_imp;\n",
    "'''\n",
    "\n",
    "for q in [q1, q2, q3]:\n",
    "    cursor.execute(q)\n",
    "    print('Count = ', cursor.fetchone()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 22s, sys: 6min 14s, total: 16min 36s\n",
      "Wall time: 25min 38s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1a345648f0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## aggregates Category Features for Searches \n",
    "\n",
    "hist_cat_agg = '''\n",
    "CREATE TABLE hist_cat_agg_daily AS\n",
    "\n",
    "WITH hist_imp AS\n",
    "(SELECT m.SearchID, AdID, SearchDate, IsClick, IsContext, Price, CategoryID CategoryID_s\n",
    "FROM last10days_merged m\n",
    ")\n",
    "\n",
    "SELECT a.CategoryID_s,\n",
    "       a.search_date,\n",
    "       cat_total_searches,\n",
    "       cat_mean_price,\n",
    "       cat_total_clicks,\n",
    "       cat_total_impressions\n",
    "        \n",
    "FROM (\n",
    "    SELECT CategoryID_s,\n",
    "           DATE(SearchDate) search_date,\n",
    "           COUNT(DISTINCT(SearchID)) cat_total_searches,\n",
    "           AVG(Price) cat_mean_price\n",
    "    FROM hist_imp\n",
    "    GROUP BY CategoryID_s, Date(SearchDate)\n",
    "    ORDER BY CategoryID_s\n",
    "     ) AS a\n",
    "     \n",
    "JOIN (\n",
    "    SELECT CategoryID_s, \n",
    "           DATE(SearchDate) search_date,\n",
    "           SUM(IsClick) cat_total_clicks,\n",
    "           COUNT(AdID) cat_total_impressions\n",
    "    FROM hist_imp\n",
    "    WHERE IsContext = 1\n",
    "    GROUP BY CategoryID_s,  Date(SearchDate)\n",
    "     ) AS b\n",
    "ON a.CategoryID_s = b.CategoryID_s\n",
    "AND a.search_date = b.search_date\n",
    ";\n",
    "'''\n",
    "%time cursor.executescript(hist_cat_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count =  32\n",
      "Count =  32\n"
     ]
    }
   ],
   "source": [
    "## sanity check to make sure there are no duplicate CategoryIDs\n",
    "\n",
    "q1 = '''\n",
    "SELECT (COUNT(DISTINCT(CategoryID_s))) FROM hist_cat_agg_daily;\n",
    "'''\n",
    "\n",
    "q2 = '''\n",
    "WITH hist_imp AS\n",
    "(SELECT m.SearchID, AdID, IsClick, IsContext, Price, CategoryID CategoryID_s\n",
    "FROM last10days_merged m\n",
    "WHERE m.SearchID IN\n",
    "    (SELECT SearchID \n",
    "     FROM hist_searches)\n",
    "     AND IsContext=1\n",
    ")\n",
    "\n",
    "SELECT COUNT(DISTINCT(CategoryID_s)) FROM hist_imp;\n",
    "'''\n",
    "\n",
    "for q in [q1, q2]:\n",
    "    cursor.execute(q)\n",
    "    print('Count = ', cursor.fetchone()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 23s, sys: 3min 37s, total: 7min 1s\n",
      "Wall time: 12min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1a345648f0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## aggregates Ad Features for Searches preceding last7searches\n",
    "## note: only for Context Ads\n",
    "hist_ad_agg = '''\n",
    "CREATE TABLE hist_ad_agg_daily AS\n",
    "\n",
    "WITH hist_imp AS (\n",
    "SELECT SearchID, SearchDate, AdID, Title, \n",
    "       IsClick, IsContext, CategoryID CategoryID_s\n",
    "FROM last10days_merged \n",
    ")\n",
    "\n",
    "SELECT\n",
    "    AdID,\n",
    "    DATE(SearchDate) search_date,\n",
    "    COUNT(SearchID) ad_total_impressions,\n",
    "    SUM(IsClick) ad_total_clicks\n",
    "\n",
    "FROM hist_imp\n",
    "WHERE IsContext = 1\n",
    "GROUP BY AdID, Date(SearchDate)\n",
    "ORDER BY AdID\n",
    ";\n",
    "'''\n",
    "%time cursor.executescript(hist_ad_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count =  27991\n",
      "Count =  27991\n"
     ]
    }
   ],
   "source": [
    "## sanity check to ensure no duplicate AdIDs\n",
    "\n",
    "q1 = '''\n",
    "SELECT (COUNT(DISTINCT(AdID))) FROM hist_ad_agg_daily;\n",
    "'''\n",
    "\n",
    "q2 = '''\n",
    "SELECT COUNT(DISTINCT(AdID)) \n",
    "FROM last10days_merged\n",
    "WHERE IsContext=1;\n",
    "'''\n",
    "for q in [q1, q2]:\n",
    "    cursor.execute(q)\n",
    "    print('Count = ', cursor.fetchone()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 51s, sys: 4min 44s, total: 8min 36s\n",
      "Wall time: 12min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1a345648f0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## aggregates User-Ad Features for Searches preceding last7searches\n",
    "## note: only for Context Ads\n",
    "hist_userad_agg = '''\n",
    "CREATE TABLE hist_userAd_agg AS\n",
    "\n",
    "WITH hist_imp AS (\n",
    "SELECT m.SearchID, AdID, UserID, IsClick, \n",
    "       IsContext, CategoryID CategoryID_s\n",
    "FROM last10days_merged m\n",
    "WHERE m.SearchID IN (\n",
    "     SELECT SearchID \n",
    "     FROM hist_searches\n",
    "                    )\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    UserID,\n",
    "    AdID,\n",
    "    COUNT(*) times_user_has_seen_ad\n",
    "FROM hist_imp\n",
    "\n",
    "WHERE IsContext = 1\n",
    "GROUP BY UserID, AdID\n",
    "ORDER BY UserID, AdID\n",
    ";\n",
    "'''\n",
    "%time cursor.executescript(hist_userad_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 17s, sys: 4min 25s, total: 7min 43s\n",
      "Wall time: 11min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1a345648f0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## user agg data from Landing Page Visits\n",
    "user_agg_visits = '''\n",
    "CREATE TABLE hist_visits_before_tr AS\n",
    "\n",
    "--selecting how many landing pages\n",
    "--a given User has visited\n",
    "SELECT hv.UserID, COUNT(hv.AdID) 'hist_user_total_visits'\n",
    "FROM hist_visits hv\n",
    "\n",
    "--join the last searchdate for each user prior to last 7 searches\n",
    "INNER JOIN (\n",
    "    SELECT UserID, MAX(SearchDate) latest_search\n",
    "    FROM hist_searches\n",
    "    GROUP BY UserID) AS maxSD\n",
    "ON hv.UserID = maxSD.UserID\n",
    "\n",
    "--only count the landing page views that precede \n",
    "--the user's last search\n",
    "WHERE hv.ViewDate < maxSD.latest_search\n",
    "GROUP BY hv.UserID; \n",
    "'''\n",
    "%time cursor.executescript(user_agg_visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating on User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_agg_processing(df_to_agg=None):\n",
    "    # pull hist_agg_set from sql\n",
    "    q = '''SELECT UserID, user_total_searches, user_category_counts,\n",
    "                 user_total_clicks, user_total_impressions\n",
    "           FROM hist_user_agg;'''\n",
    "    hist_agg_set = pd.read_sql_query(q, conn)\n",
    "    \n",
    "    # get all VisitsStream data from datetime prior to User's last 7 searches\n",
    "    q = '''SELECT UserID, hist_user_total_visits \n",
    "           FROM hist_visits_before_tr;'''\n",
    "    hist_user_visits = pd.read_sql_query(q, conn)\n",
    "    \n",
    "    # if there is no DataFrame to additionally aggregate, just merge the queries above\n",
    "    if df_to_agg is None:\n",
    "        # merge aggregated historical visits stream data\n",
    "        user_agg = hist_agg_set.merge(hist_user_visits, how='left', on='UserID')\n",
    "        \n",
    "        # the count of distinct categories a User has searched for is a proxy for how diverse\n",
    "        # their searches are\n",
    "        user_agg.rename(columns={'user_category_counts': 'user_cat_diversity'}, \n",
    "                                                                           inplace=True)\n",
    "        user_agg.fillna(0, inplace=True)\n",
    "        user_agg = user_agg.astype(int)\n",
    "        user_agg['user_HCTR'] = user_agg['user_total_clicks'] / user_agg['user_total_impressions']\n",
    "        return user_agg\n",
    "    \n",
    "    #process agg_set\n",
    "    user_agg = df_to_agg.groupby('UserID', as_index=False)\\\n",
    "                                   .agg({'SearchID': 'nunique', \n",
    "                                         'CategoryID_s': 'nunique',\n",
    "                                         'IsClick':'sum',\n",
    "                                         'AdID': 'count'})\\\n",
    "                                   .rename(columns={'SearchID':'user_total_searches', \n",
    "                                                    'CategoryID_s': 'user_category_count',\n",
    "                                                    'IsClick': 'user_total_clicks',\n",
    "                                                    'AdID': 'user_total_impressions'})\n",
    "    \n",
    "    # merge historical aggregation sets\n",
    "    user_agg = user_agg.merge(hist_user_visits, how='left', on='UserID')\n",
    "    user_agg = user_agg.merge(hist_agg_set, how='left', on='UserID', suffixes=('','_x'))\n",
    "    user_agg = user_agg.fillna(0)\n",
    "    \n",
    "    # sum up values from duplicate columns\n",
    "    user_agg['user_total_searches'] = \\\n",
    "                user_agg['user_total_searches'] + user_agg['user_total_searches_x']\n",
    "    user_agg['user_cat_diversity'] = \\\n",
    "                user_agg['user_category_count'] + user_agg['user_category_counts']\n",
    "    user_agg['user_total_impressions'] = \\\n",
    "                user_agg['user_total_impressions'] + user_agg['user_total_impressions_x']\n",
    "    user_agg['user_total_clicks'] = \\\n",
    "                user_agg['user_total_clicks'] + user_agg['user_total_clicks_x']\n",
    "    # cast datatypes as integers\n",
    "    user_agg = user_agg.astype(int)\n",
    "    \n",
    "    # create historical CTR column for User\n",
    "    user_agg['user_HCTR'] = \\\n",
    "                user_agg['user_total_clicks'] / user_agg['user_total_impressions']\n",
    "    \n",
    "    # drop duplicate columns from merge\n",
    "    drop = ['user_total_searches_x',\n",
    "            'user_category_count',\n",
    "            'user_category_counts',\n",
    "            'user_total_impressions_x',\n",
    "            'user_total_clicks_x']\n",
    "    user_agg.drop(drop, axis=1, inplace=True)\n",
    "    \n",
    "    return user_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating on Search Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_agg_processing(train_df):\n",
    "    # query\n",
    "    # pull hist_agg_set from sql\n",
    "    q= '''SELECT CategoryID_s, search_date, cat_total_searches,\n",
    "                 cat_mean_price, cat_total_clicks, cat_total_impressions \n",
    "          FROM hist_cat_agg_daily;'''\n",
    "    hist_agg_set = pd.read_sql_query(q, conn)\n",
    "    # convert to datetime\n",
    "    hist_agg_set['search_date'] = pd.to_datetime(hist_agg_set['search_date'])\n",
    "    \n",
    "    # merge historical aggregations to training set\n",
    "    cat_agg = train_df.merge(hist_agg_set, on='CategoryID_s')\n",
    "\n",
    "    # filter out rows where historical aggregations are a later date than training set impression\n",
    "    mask = cat_agg['SearchDate'].dt.date > cat_agg['search_date'].dt.date\n",
    "    \n",
    "    # group aggregates\n",
    "    cat_agg = cat_agg[mask].groupby(['SearchID', 'AdID'], as_index=False)\\\n",
    "                           .agg({'cat_total_searches' : 'sum',\n",
    "                                 'cat_mean_price' : 'mean',\n",
    "                                 'cat_total_clicks' : 'sum',\n",
    "                                 'cat_total_impressions' : 'sum'})\n",
    "    \n",
    "    # create resultant historical CTR column\n",
    "    cat_agg['cat_HCTR'] = \\\n",
    "                cat_agg['cat_total_clicks'] / cat_agg['cat_total_impressions']\n",
    "    \n",
    "    return cat_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating on AdID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ad_agg_processing(train_df):\n",
    "    # query\n",
    "    # pull ad_agg_set from sql\n",
    "    q= '''SELECT AdID, search_date, ad_total_impressions,\n",
    "                 ad_total_clicks \n",
    "          FROM hist_ad_agg_daily;'''\n",
    "    hist_agg_set = pd.read_sql_query(q, conn)\n",
    "    \n",
    "    # convert to datetime\n",
    "    hist_agg_set['search_date'] = pd.to_datetime(hist_agg_set['search_date'])\n",
    "\n",
    "    # merge historical aggregations to training set\n",
    "    ad_agg = train_df.merge(hist_agg_set, on='AdID')\n",
    "    \n",
    "    # filter out rows where historical aggregations are a later date than training set impression\n",
    "    mask = ad_agg['SearchDate'].dt.date > ad_agg['search_date'].dt.date\n",
    "    \n",
    "    # group aggregates\n",
    "    ad_agg = ad_agg[mask].groupby(['SearchID', 'AdID'], as_index=False)\\\n",
    "                         .agg({'ad_total_impressions': 'sum', \n",
    "                               'ad_total_clicks': 'sum'})\n",
    "    \n",
    "    # create resultant historical CTR column                               \n",
    "    ad_agg['ad_HCTR'] = \\\n",
    "                ad_agg['ad_total_clicks'] / ad_agg['ad_total_impressions']\n",
    "    \n",
    "    return ad_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating on User-Ad Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def userAd_agg_processing(df_to_agg=None):\n",
    "    # pull agg_set from sql\n",
    "    q= '''SELECT UserID, AdID, times_user_has_seen_ad \n",
    "          FROM hist_userAd_agg;'''\n",
    "    hist_agg_set = pd.read_sql_query(q, conn)\n",
    "    \n",
    "    # if there is no DataFrame to additionally aggregate, just return the query above\n",
    "    if df_to_agg is None:\n",
    "        return hist_agg_set.astype(int)\n",
    "    \n",
    "    # process agg_set\n",
    "    userAd_agg = df_to_agg.groupby(['UserID', 'AdID'], as_index=False)\\\n",
    "                                   .agg({'SearchID': 'count'})\\\n",
    "                                   .rename(columns={'SearchID':'times_user_has_seen_ad'})\n",
    "    \n",
    "    # merge historical aggregation sets\n",
    "    userAd_agg = userAd_agg.merge(hist_agg_set, how='left', \n",
    "                                  on=['UserID','AdID'], suffixes=('','_x'))\n",
    "    userAd_agg = userAd_agg.fillna(0)\n",
    "    \n",
    "    # sum up values from duplicate columns\n",
    "    userAd_agg['times_user_has_seen_ad'] = \\\n",
    "            (userAd_agg['times_user_has_seen_ad'] + userAd_agg['times_user_has_seen_ad_x']).astype(int)\n",
    "    \n",
    "    # drop duplicate columns from merge\n",
    "    drop = ['times_user_has_seen_ad_x']\n",
    "    userAd_agg.drop(drop, axis=1, inplace=True)\n",
    "    \n",
    "    return userAd_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_preprocessed_data(train_set, agg_set=None):\n",
    "    '''\n",
    "    Wrapper function to perform all aggregate preprocessing functions onto a dataset.\n",
    "    Effectively adds historical aggregate features to a set of observations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_set : DataFrame\n",
    "        Observations on which to merge historical agg_set data.\n",
    "        \n",
    "    agg_set : DataFrame, default None\n",
    "        Observations to aggregate and merge with SQL historical agg data.\n",
    "        If None, then processing functions will just yield the SQL aggregates,\n",
    "        and merge those to the DataFrame specified by 'train_set'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    train_set \n",
    "        Updated training set with all historical aggregate feature columns included.\n",
    "        Aggregate features comprise all searches prior to 'row_n' of train_set.\n",
    "        \n",
    "        e.g. if train_set passed was the second-to-last search for all Users, \n",
    "        then the 3rd-to-last and prior searches are aggregated together and merged.\n",
    "    '''\n",
    "    # perform all aggregate functions\n",
    "    # columns variables are specified so that later we can fill any NaNs with 0,\n",
    "    # if left merge resulted in any Nulls\n",
    "    user_agg = user_agg_processing(agg_set)\n",
    "    columns_user = list(user_agg.columns)\n",
    "    \n",
    "    cat_agg = cat_agg_processing(train_set)\n",
    "    columns_cat = list(cat_agg.columns)\n",
    "    \n",
    "    ad_agg = ad_agg_processing(train_set)\n",
    "    columns_ad = list(ad_agg.columns)\n",
    "    \n",
    "    user_ad_agg = userAd_agg_processing(agg_set)\n",
    "    columns_user_ad = list(user_ad_agg.columns)\n",
    "    \n",
    "    # merge aggregate DataFrames onto the set of observations, i.e. 'train_set'\n",
    "    train_set = train_set.merge(user_agg, how= 'left', on='UserID')\n",
    "    train_set[columns_user] = train_set[columns_user].fillna(0)\n",
    "    \n",
    "    # for all the merges below, it's necessary to merge on the level \n",
    "    # of an *impression* (SearchID+AdID) or (UserID+AdID)\n",
    "    train_set = train_set.merge(cat_agg, how='left', on=['SearchID', 'AdID']) \n",
    "    train_set[columns_cat] = train_set[columns_cat].fillna(0)\n",
    "    \n",
    "    train_set = train_set.merge(ad_agg, how='left', on=['SearchID', 'AdID'])\n",
    "    train_set[columns_ad] = train_set[columns_ad].fillna(0)\n",
    "    \n",
    "    train_set = train_set.merge(user_ad_agg, how='left', on=['UserID', 'AdID'])\n",
    "    train_set[columns_user_ad] = train_set[columns_user_ad].fillna(0)\n",
    "    \n",
    "    return train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SearchID</th>\n",
       "      <th>AdID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>SearchDate</th>\n",
       "      <th>Price</th>\n",
       "      <th>Position</th>\n",
       "      <th>title_length</th>\n",
       "      <th>IsClick</th>\n",
       "      <th>IsUserLoggedOn</th>\n",
       "      <th>CategoryID_s</th>\n",
       "      <th>CategoryID_a</th>\n",
       "      <th>row_n</th>\n",
       "      <th>common_params_cnt</th>\n",
       "      <th>categories_match</th>\n",
       "      <th>blank_query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>13524889</td>\n",
       "      <td>3310798</td>\n",
       "      <td>2015-05-15 17:38:46</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>34084553</td>\n",
       "      <td>3310798</td>\n",
       "      <td>2015-05-15 17:38:46</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>12281759</td>\n",
       "      <td>3524741</td>\n",
       "      <td>2015-05-13 09:58:44</td>\n",
       "      <td>40086.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>29614929</td>\n",
       "      <td>4114490</td>\n",
       "      <td>2015-05-15 09:56:17</td>\n",
       "      <td>9829.0</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>19880051</td>\n",
       "      <td>3314799</td>\n",
       "      <td>2015-05-18 08:46:10</td>\n",
       "      <td>17100.0</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SearchID      AdID   UserID          SearchDate    Price  Position  \\\n",
       "0        10  13524889  3310798 2015-05-15 17:38:46   5000.0         1   \n",
       "1        10  34084553  3310798 2015-05-15 17:38:46  30000.0         7   \n",
       "2        23  12281759  3524741 2015-05-13 09:58:44  40086.0         1   \n",
       "3        25  29614929  4114490 2015-05-15 09:56:17   9829.0         1   \n",
       "4        37  19880051  3314799 2015-05-18 08:46:10  17100.0         7   \n",
       "\n",
       "   title_length  IsClick  IsUserLoggedOn  CategoryID_s  CategoryID_a  row_n  \\\n",
       "0            38        0               0            34            12      5   \n",
       "1            12        0               0            34            12      5   \n",
       "2            40        0               0            38            38      5   \n",
       "3            47        0               0            47            47      5   \n",
       "4            28        0               0            11            11      3   \n",
       "\n",
       "   common_params_cnt  categories_match  blank_query  \n",
       "0                  0                 0            1  \n",
       "1                  0                 0            1  \n",
       "2                  1                 1            1  \n",
       "3                  0                 1            0  \n",
       "4                  0                 1            1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review of train_processed DataFrame (7th-to-last to 2nd-to-last Searches Per User)\n",
    "train_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Features and Scalers/Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features to use, separated by type\n",
    "# Note: categorical features should be binned together, since these will feed a One-Hot-Encoder\n",
    "features_ = {'basic_features' : ['Price', 'title_length',  'common_params_cnt'],\n",
    "                    \n",
    "             'user_features' : ['user_total_searches', 'user_cat_diversity', 'user_total_impressions', \n",
    "                                       'user_total_clicks', 'hist_user_total_visits', 'user_HCTR'],\n",
    "\n",
    "             'cat_features' : ['cat_total_searches', 'cat_mean_price', 'cat_total_clicks', \n",
    "                                      'cat_total_impressions', 'cat_HCTR'],\n",
    "\n",
    "             'ad_features' : ['ad_total_impressions', 'ad_total_clicks', 'ad_HCTR'],\n",
    "\n",
    "             'user_ad_features' : ['times_user_has_seen_ad'],\n",
    "\n",
    "             'categorical' : ['Position', 'CategoryID_a', 'IsUserLoggedOn', 'blank_query', 'categories_match']}\n",
    "\n",
    "# concatenate features\n",
    "features = features_['basic_features'] + features_['categorical'] + features_['user_features'] + \\\n",
    "           features_['cat_features'] + features_['ad_features'] + features_['user_ad_features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with including/excluding some of these features by removing individual columns from the dictionary, or subsets from the concatenation statement.**\n",
    "\n",
    "If you want to reset to default features you can load the dictionary again from the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below for reset features_ variable to default\n",
    "\n",
    "# with open('default_features.json', 'r') as file:\n",
    "#     features_ = json.loads(file)\n",
    "#     features = features_['basic_features'] + features_['categorical'] + features_['user_features'] + \\\n",
    "#                features_['cat_features'] + features_['ad_features'] + features_['user_ad_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up StandardScaler and OneHotEncoder pipeline for feature scaling/encoding\n",
    "preprocess = make_column_transformer((StandardScaler(), features),\n",
    "                                     (OneHotEncoder(categories='auto', \n",
    "                                                   handle_unknown='ignore'), features_['categorical'])\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_models(train_df, train_val_dict, features, n_fold, models, preprocessor=preprocess, **kwargs):\n",
    "    \n",
    "    # dictionary to save auc scores per model per round\n",
    "    scores = defaultdict(list)\n",
    "    for i in range(1, n_fold+1):\n",
    "        # get indices for splits\n",
    "        agg_v_index = train_val_dict[f'agg_set_v{i}']\n",
    "        t_index = train_val_dict[f'train_set_{i}']\n",
    "        v_index = train_val_dict[f'val_set_{i}']\n",
    "        \n",
    "        # filter full train_df for splits\n",
    "        agg_set_v = train_df.iloc[agg_v_index]\n",
    "        train_set = train_df.iloc[t_index]\n",
    "        val_set = train_df.iloc[v_index]\n",
    "        \n",
    "        # merge aggregated data\n",
    "        # Note: in Round 1, there won't be any agg_set for training\n",
    "        if i == 1:\n",
    "            t = merge_preprocessed_data(train_set)\n",
    "            v = merge_preprocessed_data(val_set, agg_set_v)\n",
    "        # all other rounds will include agg_set_t in merge_preprocessed step on 't'\n",
    "        else:\n",
    "            agg_t_index = train_val_dict[f'agg_set_t{i}']\n",
    "            agg_set_t = train_df.iloc[agg_t_index]\n",
    "            t = merge_preprocessed_data(train_set, agg_set_t)\n",
    "            v = merge_preprocessed_data(val_set, agg_set_v)\n",
    "            \n",
    "        # split train into features(X) and target(y)\n",
    "        t['Price'] = t['Price'].fillna(0)\n",
    "        X_t = t[features]\n",
    "        X_ts = preprocessor.fit_transform(X_t)\n",
    "        y_t = t.IsClick\n",
    "        \n",
    "        # split val into features(X) and target(y)\n",
    "        v['Price'] = v['Price'].fillna(0)\n",
    "        X_v = v[features]\n",
    "        X_vs = preprocessor.transform(X_v)\n",
    "        y_v = v.IsClick\n",
    "        \n",
    "        print(f'Cross-Validation Round {i} ROC-AUC')\n",
    "        print('*' * 20)\n",
    "        \n",
    "        # iterate through models and fit/predict\n",
    "        for model in models:\n",
    "            name = model.__class__.__name__\n",
    "            f_start = time.time()\n",
    "            # fit on scaled/encoded training set, kwargs option available if using xgboost\n",
    "            model.fit(X_ts, y_t, **kwargs)\n",
    "            f_end = time.time()\n",
    "            fit_duration = round(f_end - f_start, 2)\n",
    "            \n",
    "            p_start = time.time()\n",
    "            # predict probability of target class for scaled/encoded validation set\n",
    "            preds = model.predict_proba(X_vs)[:,1]\n",
    "            p_end = time.time()\n",
    "            # get AUC score\n",
    "            auc = roc_auc_score(y_v, preds)\n",
    "            pred_duration = round(p_end - p_start, 2)\n",
    "            \n",
    "            print(f'{name} = {auc}')\n",
    "            print(f'Fit Time = {fit_duration} sec; Predict Time = {pred_duration} sec')\n",
    "            # add AUC score to scores dict for model key\n",
    "            scores[name].extend([float(auc)])\n",
    "        print('\\n')\n",
    "    \n",
    "    # return the dictionary of AUC scores\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set models to iterate through\n",
    "models = [LogisticRegression(solver='liblinear'),\n",
    "          DecisionTreeClassifier(max_depth=5, random_state=666),\n",
    "          RandomForestClassifier(n_estimators=150, max_depth=3, n_jobs=-1, random_state=666)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might also experiment with using the same class of model, but different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Round 1 ROC-AUC\n",
      "********************\n",
      "LogisticRegression = 0.7468582726755095 in 82.35 seconds\n",
      "DecisionTreeClassifier = 0.7310512834279119 in 51.36 seconds\n",
      "RandomForestClassifier = 0.7343459341071017 in 335.28 seconds\n",
      "\n",
      "\n",
      "Cross-Validation Round 2 ROC-AUC\n",
      "********************\n",
      "LogisticRegression = 0.7456183712628208 in 94.18 seconds\n",
      "DecisionTreeClassifier = 0.7338041532987283 in 53.26 seconds\n",
      "RandomForestClassifier = 0.7368761581583327 in 330.63 seconds\n",
      "\n",
      "\n",
      "Cross-Validation Round 3 ROC-AUC\n",
      "********************\n",
      "LogisticRegression = 0.7501811564318067 in 138.68 seconds\n",
      "DecisionTreeClassifier = 0.7375553518372434 in 63.15 seconds\n",
      "RandomForestClassifier = 0.7394734907290246 in 383.12 seconds\n",
      "\n",
      "\n",
      "Cross-Validation Round 4 ROC-AUC\n",
      "********************\n",
      "LogisticRegression = 0.7533457678363117 in 119.7 seconds\n",
      "DecisionTreeClassifier = 0.7420823633009463 in 77.12 seconds\n",
      "RandomForestClassifier = 0.7383647888574435 in 447.07 seconds\n",
      "\n",
      "\n",
      "Cross-Validation Round 5 ROC-AUC\n",
      "********************\n",
      "LogisticRegression = 0.7601918748851038 in 88.0 seconds\n",
      "DecisionTreeClassifier = 0.7507453670951381 in 96.48 seconds\n",
      "RandomForestClassifier = 0.7439993643284862 in 557.85 seconds\n",
      "\n",
      "\n",
      "CPU times: user 1h 37min 59s, sys: 10min 39s, total: 1h 48min 39s\n",
      "Wall time: 1h 25min 32s\n"
     ]
    }
   ],
   "source": [
    "# run the validation function\n",
    "%time cv_scores = validate_models(train_processed, train_val_dict, features, 5, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression performs the best! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot changes in AUC for various feature sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "    \n",
    "fpr, tpr, _ = roc_curve(basic_f['Round 5'][0], basic_f['Round 5'][1])\n",
    "plt.plot(fpr,tpr, label='basic')\n",
    "fpr, tpr, _ = roc_curve(cat_f['Round 5'][0], cat_f['Round 5'][1])\n",
    "plt.plot(fpr,tpr, label='categories')\n",
    "fpr, tpr, _ = roc_curve(user_f['Round 5'][0], user_f['Round 5'][1])\n",
    "plt.plot(fpr,tpr, label='user')\n",
    "fpr, tpr, _ = roc_curve(ad_f['Round 5'][0], ad_f['Round 5'][1])\n",
    "plt.plot(fpr,tpr, label='ad')\n",
    "fpr, tpr, _ = roc_curve(user_cat_f['Round 5'][0], user_cat_f['Round 5'][1])\n",
    "plt.plot(fpr,tpr, ':',label='user_category')\n",
    "fpr, tpr, _ = roc_curve(cat_ad_f['Round 5'][0], cat_ad_f['Round 5'][1])\n",
    "plt.plot(fpr,tpr, ':',label='category_ad')\n",
    "fpr, tpr, _ = roc_curve(user_ad_f['Round 5'][0], user_ad_f['Round 5'][1])\n",
    "plt.plot(fpr,tpr, ':', label='user_ad')\n",
    "fpr, tpr, _ = roc_curve(all_features['Round 5'][0], all_features['Round 5'][1])\n",
    "plt.plot(fpr,tpr, '-.', color='black', label='all', linewidth= 5)\n",
    "plt.plot([0,1], '--y')\n",
    "plt.legend()\n",
    "plt.title('ROC Per Additional Feature')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.savefig('ROC_features.jpg', dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval(train_df, test_df, train_val_dict, features, model, preprocessor=preprocess, **kwargs):\n",
    "    \n",
    "    # get indices for splits\n",
    "    agg_tr_index = train_val_dict[f'agg_set_v5']\n",
    "    t_index = train_val_dict[f'val_set_5']\n",
    "\n",
    "    # filter full train_df for splits\n",
    "    agg_set_tr = train_df.iloc[agg_tr_index]\n",
    "    train = train_df.iloc[t_index]\n",
    "    test = test_df\n",
    "    \n",
    "    # merge aggregated data    \n",
    "    train = merge_preprocessed_data(train, agg_set_tr)\n",
    "    test = merge_preprocessed_data(test, train_df)\n",
    "    print('Finished Preprocessing')\n",
    "    \n",
    "    # split train into features(X) and target(y)\n",
    "    train['Price'] = train['Price'].fillna(0)\n",
    "    X_tr = train[features]\n",
    "    X_tr = preprocessor.fit_transform(X_tr)\n",
    "    y_tr = train.IsClick\n",
    "    \n",
    "    # split test into features(X) and target(y)\n",
    "    test['Price'] = test['Price'].fillna(0)\n",
    "    X_te = test[features]\n",
    "    X_te = preprocessor.transform(X_te)\n",
    "    y_te = test.IsClick\n",
    "    \n",
    "    print('Starting Model Training')\n",
    "    # fit on scaled/encoded training set, kwargs option available if using xgboost\n",
    "    model.fit(X_tr, y_tr, **kwargs)\n",
    "    print('Done Training')\n",
    "    print('Generating Predictions')\n",
    "    # predict probability of target class for scaled/encoded test set\n",
    "    preds = model.predict_proba(X_te)[:,1]\n",
    "    # get AUC score\n",
    "    auc = roc_auc_score(y_te, preds)\n",
    "    print(f'Hold-out AUC score = {auc}')\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model for testing\n",
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold-out AUC score = 0.7553444903814677\n",
      "CPU times: user 7min 12s, sys: 3min 53s, total: 11min 5s\n",
      "Wall time: 10min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.0125739 , 0.11579401, 0.00193943, ..., 0.00115157, 0.00234092,\n",
       "       0.00595754])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run test function\n",
    "%time test_eval(train_processed, test_processed, train_val_dict, features, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle = pd.read_pickle(open('pickle/kaggle_processed.pkl.gzip', 'rb'), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_kaggle_df(kaggle_set):\n",
    "    '''\n",
    "    Ensures that kaggle_dataframe is correct type and shape.\n",
    "    '''\n",
    "    if type(kaggle_set) != type(pd.DataFrame()):\n",
    "        raise TypeError('\"kaggle_set\" parameter must be a DataFrame')\n",
    "    if kaggle_set.shape != (7816361, 14):\n",
    "        raise Exception('Kaggle DataFrame is the wrong size. Should be 7816361x14.')\n",
    "        # columns should be: TestID, SearchID, AdID, UserID, SearchDate, Price,\n",
    "        #                    Position, title_length, IsUserLoggedOn, CategoryID_s,\n",
    "        #                    CategoryID_a, common_params_cnt, categories_match, blank_query\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_kaggle(train_df, test_df, kaggle_set, train_val_dict, features, model, preprocessor=preprocess, **kwargs):\n",
    "    \n",
    "    # ensure that kaggle_set is DataFrame with correct shape\n",
    "    check_kaggle_df(kaggle_set)\n",
    "    \n",
    "    # set train/test/agg sets\n",
    "    agg_set_tr = train_df\n",
    "    train = test_df\n",
    "    test = kaggle_set\n",
    "    \n",
    "    # merge aggregated data    \n",
    "    train = merge_preprocessed_data(train, agg_set_tr)\n",
    "    test = merge_preprocessed_data(test, pd.concat([train_df, test_df]))\n",
    "    print('Finished Preprocessing')\n",
    "    \n",
    "    # split train into features(X) and target(y)\n",
    "    train['Price'] = train['Price'].fillna(0)\n",
    "    X_tr = train[features]\n",
    "    X_tr = preprocess.fit_transform(X_tr)\n",
    "    y_tr = train.IsClick\n",
    "    \n",
    "    # split test into features(X) and target(y)\n",
    "    test['Price'] = test['Price'].fillna(0)\n",
    "    X_te = test[features]\n",
    "    X_te = preprocess.transform(X_te)\n",
    "    # Note: there is no target(y) in the kaggle dataframe\n",
    "    \n",
    "    print('Starting Model Training')\n",
    "    # fit on scaled/encoded training set, kwargs option available if using xgboost\n",
    "    model.fit(X_tr, y_tr, **kwargs)\n",
    "    print('Done Training')\n",
    "    print('Generating Predictions')\n",
    "    # predict probability of target class for scaled/encoded kaggle set\n",
    "    preds = m.predict_proba(X_te)[:,1]\n",
    "    \n",
    "    print('Done')\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model with desired hyperparameters\n",
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished preprocessing\n",
      "starting fit\n",
      "done fitting\n",
      "CPU times: user 40min 51s, sys: 5min 13s, total: 46min 5s\n",
      "Wall time: 47min 39s\n"
     ]
    }
   ],
   "source": [
    "# run kaggle function\n",
    "%time preds = predict_kaggle(train_processed, test_processed, train_val_dict, features, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set click predictions as column\n",
    "kaggle['IsClick'] = preds\n",
    "# write to csv - this is the correct format for submission\n",
    "kaggle[['TestId', 'IsClick']].to_csv(open('submission.csv', 'w'), header=['ID', 'IsClick'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: Keep in mind that we've been using AUC to evaluate rather than the competition metric - LogLoss.\n",
    "\n",
    "That said, the above gets you around the Top 120 (of 414) or so submissions on the Private Leaderboard with a LogLoss ~0.04751.  Removing some of the features can improve that score a little bit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
